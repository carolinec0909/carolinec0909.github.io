---
title: "Hierarchical Softmax for Large-Class Recommendation Problems"
date: 2025-11-17
---

Modern recommendation systems must model user interests across **extremely large vocabularies**. A single user may interact with tens of thousands of products, categories, annotations, or latent interest tags. If we want to build user embeddings from these signals, one natural approach is to model a probability distribution over all items a user could be interested in.

This requires computing a **softmax** over a very large item vocabulary. When the number of classes is large, this softmax becomes the computational bottleneck. **Hierarchical Softmax (HSM)** solves this by replacing one large softmax with several smaller ones.

---

## The Problem: Softmax Over Many Classes

Assume we want to compute the probability of an item given a user-context vector $x$:

$$P(\text{item} \mid x) = \text{softmax}(xW + b)$$

Where:
- $x \in \mathbb{R}^d$ is the **user embedding**
- $W \in \mathbb{R}^{n \times d}$ is the **item embedding matrix**
- $n$ is the **number of items** (the vocabulary size)
- $d$ is the **embedding dimension**

To compute logits for all $n$ items, we must compute one dot product per item:

$$\text{logit}_i = x \cdot W_i + b_i$$

Each dot product $x \cdot W_i$ requires $d$ multiplications and $d-1$ additions.

### Total Cost of Normal Softmax

$$\text{Cost} \approx nd + n \approx O(nd)$$

This cost appears in **every training step** and dominates compute when $n$ is large.

---

## A Simple Example: Split the Vocabulary Into Two Groups

Imagine splitting the vocabulary into two groups ($g_1, g_2$) of size $n/2$ each.

1. Predict the group (softmax over 2 choices) → cost $O(d)$  
2. Predict the item inside the chosen group (softmax over $n/2$ items) → cost $O(nd/2)$

Total cost drops to roughly $O(nd/2)$ — already a 2× speedup.

---

## Extending the Idea: Hierarchical Softmax

Recursively splitting groups yields a **binary tree**. Leaves are items; each internal node performs a small binary softmax deciding which subtree to enter.

For a vocabulary of size $n$, a balanced tree has depth $\lceil \log_2 n \rceil$.  
To score a single item, we only evaluate the $\log_2 n$ nodes on the root-to-leaf path.

### Final Complexity

| Method                  | Cost per item prediction |
|-------------------------|--------------------------|
| Standard softmax        | $O(nd)$                  |
| Hierarchical Softmax    | $O(d \log n)$            |

This reduction from linear to logarithmic in vocabulary size is what makes HSM practical for millions of items.

---

## Why HSM Fits Recommendation Systems

Modern recsys deal with massive catalogs: products, videos, tags, categories, etc. Modeling user preferences as a full distribution over such catalogs is exactly the large-output-space problem HSM was designed to solve.

### Key Benefits

- Training stays fast even with millions of items
- Memory-efficient (no need to materialize the full $n \times d$ matrix at once)
- Scales gracefully as catalogs grow
- Proven at scale (Word2Vec, early YouTube models, etc.)

---

## Summary

**Hierarchical Softmax** replaces one giant $O(nd)$ softmax with a tree of tiny softmaxes, reducing per-item cost to $O(d \log n)$.

For any recommendation problem involving huge item vocabularies, HSM remains one of the simplest and most effective ways to keep training fast and memory usage reasonable.

---

Would you like to explore how the tree is usually built (Huffman coding vs balanced vs learned hierarchies) or see a compact PyTorch implementation?