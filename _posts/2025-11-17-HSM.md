---
title: "Hierarchical Softmax for Large-Class Recommendation Problems"
date: 2025-11-17
---

Modern recommendation systems must model user interests across extremely large vocabularies. A single user may interact with tens of thousands of products, categories, annotations, or latent interest tags. If we want to build user embeddings from these signals, one natural approach is to model a probability distribution over all items a user could be interested in.

This requires computing a softmax over a very large item vocabulary. When the number of classes is large, this softmax becomes the computational bottleneck. Hierarchical Softmax (HSM) solves this by replacing one large softmax with several smaller ones.

---

# The Problem: Softmax Over Many Classes

Assume we want to compute:

\[
P(\text{item} \mid x) = \text{softmax}(xW + b)
\]

Where:
- \(x \in \mathbb{R}^d\) is the user embedding  
- \(W \in \mathbb{R}^{n \times d}\) is the item embedding matrix  
- \(n\) is the number of items  
- \(d\) is the embedding dimension  

To compute logits for all items, we must compute one dot product per item.

\[
\text{logit}_i = x \cdot W_i + b_i
\]

Each dot product is:

\[
x \cdot W_i = \sum_{j=1}^{d} x_j W_{i,j}
\]

This requires:
- \(d\) multiplications  
- \(d - 1\) additions  

## Total cost of normal softmax

\[
n \text{ dot products of size } d \;+\; n \text{ bias adds}
\]

\[
nd + n \approx O(nd)
\]

This cost appears in every training step and dominates compute when \(n\) is large.

---

# A Simple Example: Split the Vocabulary Into Two Groups

To build intuition, imagine splitting the vocabulary into two groups of equal size \( n/2 \).

## Step 1: Predict the group

\[
P(g \mid x) = \text{softmax}(xW_g + b_g)
\]

This is a small softmax over 2 groups.  
Cost:

\[
2d + 2
\]

## Step 2: Predict the item inside the selected group

\[
P(w \mid x, g) = \text{softmax}(xW_{\text{group}} + b_{\text{group}})
\]

This is a softmax over \(n/2\) items.  
Cost:

\[
\frac{n}{2}d + \frac{n}{2}
\]

## Total cost after the split

\[
(2 + \tfrac{n}{2})d + 2 + \tfrac{n}{2}
\]

Compare with the original:

\[
nd + n
\]

This gives a roughly two times speedup.

---

# Extending the Idea: Hierarchical Softmax

If we continue splitting groups into smaller groups, we obtain a tree. Each internal node represents a softmax over a few choices, and each leaf corresponds to an item.

If we use a binary tree, the depth is:

\[
\log_2(n)
\]

At each level, we compute a softmax over 2 choices. This means the number of dot products required for an item prediction becomes:

\[
2 \log_2(n)
\]

Each dot product is size \(d\).

## Final complexity of Hierarchical Softmax

\[
O(d \log n)
\]

This is a major improvement over the original \(O(nd)\).

---

# ASCII Diagram of Hierarchical Softmax

The diagram below shows how HSM represents a vocabulary of 8 items using a binary tree. To compute the probability of a single item, we traverse one path from root to leaf and compute a small softmax at each internal node.


                (root)
               /      \
        softmax(2)   softmax(2)
          /   \         /    \
    softmax(2) softmax(2)  softmax(2) softmax(2)
      /  \         /  \         /  \        /   \
    w1   w2      w3   w4      w5   w6     w7    w8


Each internal node computes a small softmax.  
To compute the probability of item w6, we evaluate only the nodes along the path:

root → right → left → w6

Cost:

\[
\log_2(8) = 3 \text{ levels}
\]

Each level requires a softmax over 2 nodes.

This reduces computation from evaluating 8 dot products to evaluating only 3 small softmaxes.

---

# Why HSM Fits Recommendation Systems

In recommendation systems, the number of possible items can be extremely large:
- products  
- content IDs  
- interest annotations  
- user tags  
- category labels  

Modeling user preference distributions over all of these requires computing an output distribution over many classes. This is exactly the kind of large-class problem for which HSM is designed.

Benefits:
- Efficient training when the item vocabulary is large  
- Scales to hundreds of thousands or millions of items  
- Reduces training cost from \(O(nd)\) to \(O(d \log n)\)  
- Avoids computing dot products for every item in the vocabulary  

HSM makes training feasible even when we want to incorporate rich item inventories and dense user interest features.

---

# Summary

Hierarchical Softmax replaces a large softmax over \(n\) items with a sequence of small softmaxes along a tree path. This reduces computation from:

\[
O(nd)
\]

to:

\[
O(d \log n)
\]

For large-class recommendation problems, this provides a scalable way to model probability distributions over huge item spaces while keeping training efficient.

