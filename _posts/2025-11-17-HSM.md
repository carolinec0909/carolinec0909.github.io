---
title: "Hierarchical Softmax for Large-Class Recommendation Problems"
date: 2025-11-17
---

Modern recommendation systems must model user interests across **extremely large vocabularies**. A single user may interact with tens of thousands of products, categories, annotations, or latent interest tags. If we want to build user embeddings from these signals, one natural approach is to model a probability distribution over all items a user could be interested in.

This requires computing a **softmax** over a very large item vocabulary. When the number of classes is large, this softmax becomes the computational bottleneck. **Hierarchical Softmax (HSM)** solves this by replacing one large softmax with several smaller ones.

---

## The Problem: Softmax Over Many Classes

Assume we want to compute the probability of an item given a user-context vector $x$:

$$
P(\text{item} \mid x) = \text{softmax}(xW + b)
$$

Where:
* $x \in \mathbb{R}^d$ is the **user embedding**
* $W \in \mathbb{R}^{n \times d}$ is the **item embedding matrix**
* $n$ is the **number of items** (the vocabulary size)
* $d$ is the **embedding dimension**

To compute logits for all $n$ items, we must compute one dot product per item:

$$
\text{logit}_i = x \cdot W_i + b_i
$$

Each dot product $x \cdot W_i$ requires $d$ multiplications and $d-1$ additions.

### Total Cost of Normal Softmax

The computation involves $n$ dot products of size $d$, plus $n$ bias additions.

$$
\text{Cost} \approx n d + n \approx O(nd)
$$

This cost appears in **every training step** and dominates the compute when $n$ (the vocabulary size) is large.

---

## A Simple Example: Split the Vocabulary Into Two Groups

To build intuition, imagine splitting the vocabulary into two groups ($g_1, g_2$) of equal size $n/2$.

### 1. Predict the Group

We first compute a small softmax over the 2 groups:

$$
P(g \mid x) = \text{softmax}(xW_g + b_g)
$$

* **Cost:** $2d + 2 \approx O(d)$

### 2. Predict the Item inside the Selected Group

We then compute a softmax over the $n/2$ items within the selected group:

$$
P(w \mid x, g) = \text{softmax}(xW_{\text{group}} + b_{\text{group}})
$$

* **Cost:** $\frac{n}{2}d + \frac{n}{2} \approx O(nd/2)$

### Total Cost after the Split

$$
\text{Cost}_{\text{Split}} \approx (2 + \tfrac{n}{2})d + 2 + \tfrac{n}{2}
$$

Comparing $O(nd/2)$ to the original $O(nd)$, this simple split already gives a roughly **two times speedup** by avoiding half of the dot products.

---

## Extending the Idea: Hierarchical Softmax

If we continue splitting groups into smaller groups, we obtain a **binary tree structure**. Each internal node represents a small softmax over its children, and each leaf corresponds to an item.



If the vocabulary size is $n$, using a binary tree, the depth of the tree is:

$$
\text{Depth} = \log_2(n)
$$

To compute the probability of a single item, we only need to traverse the path from the root to that item's leaf. At each of the $\log_2(n)$ levels, we only compute a softmax over **two** choices. This means the number of dot products required for a single item prediction is:

$$
\text{Dot Products} = \log_2(n) \text{ or } 2 \log_2(n) \text{ (depending on the implementation)}
$$

Since each dot product is of size $d$, the total computational complexity dramatically decreases.

### Final Complexity of Hierarchical Softmax

The computational cost for a single item probability reduces from $O(nd)$ to:

$$
O(d \log n)
$$

This is a **major improvement** over the original $O(nd)$.

---

## ASCII Diagram of Hierarchical Softmax

The diagram below shows how HSM represents a vocabulary of 8 items using a binary tree. To compute the probability of a single item, we traverse one path from root to leaf and compute a small softmax at each internal node.


                (root)
               /      \
        softmax(2)   softmax(2)
          /   \         /    \
    softmax(2) softmax(2)  softmax(2) softmax(2)
      /  \         /  \         /  \        /   \
    w1   w2      w3   w4      w5   w6     w7    w8


To compute the probability of item **w6**, we evaluate only the nodes along the path:

$$
\text{root} \rightarrow \text{right} \rightarrow \text{left} \rightarrow \text{w6}
$$

* **Cost:** $\log_2(8) = 3$ levels.
* We evaluate only 3 small softmaxes, not 8 large dot products.

---

## Why HSM Fits Recommendation Systems

In modern recommendation systems, the number of possible items can be extremely large: **products, content IDs, user tags, category labels**, etc.

Modeling user preference distributions over all of these requires computing an output distribution over many classes. This is exactly the kind of **large-class problem** for which HSM is designed.

### Key Benefits

* **Efficient Training:** Makes training feasible when the item vocabulary ($n$) is huge.
* **Scalability:** Scales easily to hundreds of thousands or even millions of items.
* **Cost Reduction:** Reduces training cost from $O(nd)$ to $O(d \log n)$.
* **Efficiency:** Avoids computing dot products for every item in the vocabulary.

HSM makes training feasible even when we want to incorporate rich item inventories and dense user interest features.

---

## Summary

**Hierarchical Softmax** replaces a large softmax over $n$ items with a sequence of small softmaxes along a tree path. This reduces computation from:

$$
O(nd)
$$

to:

$$
O(d \log n)
$$

For large-class recommendation problems, this provides a scalable way to model probability distributions over huge item spaces while keeping training efficient.

---

Would you like to explore how the tree structure for HSM is typically learned or pre-defined?